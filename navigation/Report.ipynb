{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning Nanodegree - Project 1: Navigation Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Summary\n",
    "\n",
    "For this project, an agent was trained to navigate in a large square world and collect yellow bananas.\n",
    "\n",
    "A reward of +1 is provided for collecting a yellow banana, and a reward of -1 is provided for collecting a blue banana. The goal was to get an average score of +13 over 100 consecutive episodes.\n",
    "\n",
    "The state space has 37 dimensions and contains the agent's velocity, along with ray-based perception of objects around the agent's forward direction. Four discrete actions are available: move forward, move backward, turn left, turn right.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "#### Agent and Q-Network\n",
    "The agent is initialised in dqn_agent.py with state and action size. The goal is to find the optimal action-value function that maximises the cumulative reward. In this case a Deep Q-Learning algorithm represents the action-value function instead of a Q-table. The agent uses two Q-Networks to solve the environment, a target and a local network(defined in model.py).\n",
    "\n",
    "The neural network consists of three hidden layers and all three layers have\n",
    "a ReLU activation function. The network takes 37 states as inputs and outputs the action\n",
    "value function for every possible action in the corresponding state.\n",
    "\n",
    "\n",
    "#### Training\n",
    "After resetting the environment the agent chooses an action, the next state, reward and done parameters are returned. The agent saves the observed experience in replay memory and takes the next step based on the information received. Once the agent has enough samples available in memory, it gets a random subset and starts to learn from that batch using gradient descent update step.\n",
    "\n",
    "The agent was expected to solve the environment under 1800 episodes and it was able to do so under 600 episodes. The received rewards after each episode can be seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rewards](./rewards.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters used to achieve this result:\n",
    "eps_start : 1\n",
    "\n",
    "eps_end:  0.1\n",
    "\n",
    "eps_decay:  0.995\n",
    "\n",
    "BATCH_SIZE: 64\n",
    "\n",
    "Gamma: 0.99\n",
    "\n",
    "TAU: 2e -3\n",
    "\n",
    "Learning Rate:  1e -3\n",
    "\n",
    "The model weights of the successful agent can be found in checkpoint.pth saved.\n",
    "\n",
    "#### Ideas for future work\n",
    "To improve the original Deep Q-Learning algorithm, combining Double DQN, Prioritized experience replay, Dueling DQN, Multi-Step Bootstrap Targets, Distributional DQN and Noisy DQN into one Rainbow agent was shown to outperform the individual algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
